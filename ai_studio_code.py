import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import io
import chardet
from datetime import date, datetime
import urllib3
import yfinance as yf
import calendar

# -------------------------------------------
# 1. åŸºç¤è¨­å®š
# -------------------------------------------
st.set_page_config(page_title="å°è‚¡æŒ‡æ•¸æˆ°æƒ…å®¤ Pro", layout="wide")
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# -------------------------------------------
# 2. çˆ¬èŸ²èˆ‡æ•¸æ“šç²å– (å¢å¼·ç‰ˆ)
# -------------------------------------------
@st.cache_data(ttl=3600)
def fetch_taifex_rankings(limit=200):
    # (ç¶­æŒåŸæ¨£ï¼Œçœç•¥é‡è¤‡ä»£ç¢¼ä»¥ç¯€çœç¯‡å¹…ï¼Œè«‹ä¿ç•™åŸæœ¬çš„ fetch_taifex_rankings é‚è¼¯)
    url = "https://www.taifex.com.tw/cht/9/futuresQADetail"
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        encoding = chardet.detect(resp.content)["encoding"] or resp.apparent_encoding
        html_text = resp.content.decode(encoding, errors="ignore")
        soup = BeautifulSoup(html_text, "lxml")
        rows = []
        for tr in soup.find_all("tr"):
            tds = tr.find_all("td")
            if not tds: continue
            rank, code, name = None, None, None
            txts = [td.get_text(strip=True) for td in tds]
            for s in txts:
                if rank is None and re.fullmatch(r"\d+", s): rank = int(s)
                elif rank and not code and re.fullmatch(r"\d{4}", s): code = s
                elif rank and code and not name and not re.fullmatch(r"\d+", s):
                    name = s; break
            if rank and code and name: rows.append({"æ’å": rank, "è‚¡ç¥¨ä»£ç¢¼": code, "è‚¡ç¥¨åç¨±": name})
        if not rows:
            dfs = pd.read_html(io.StringIO(html_text), flavor=["lxml", "html5lib"])
            for df in dfs:
                cols = "".join([str(c) for c in df.columns])
                if "æ’å" in cols and ("åç¨±" in cols or "ä»£è™Ÿ" in cols):
                    df.columns = [str(c).replace(" ", "") for c in df.columns]
                    col_map = {c: ("æ’å" if "æ’å" in c else "è‚¡ç¥¨ä»£ç¢¼" if "ä»£" in c else "è‚¡ç¥¨åç¨±") for c in df.columns if any(x in c for x in ["æ’å","ä»£","å"])}
                    df = df.rename(columns=col_map)
                    df = df[pd.to_numeric(df["æ’å"], errors='coerce').notnull()]
                    df["æ’å"] = df["æ’å"].astype(int)
                    df["è‚¡ç¥¨ä»£ç¢¼"] = df["è‚¡ç¥¨ä»£ç¢¼"].astype(str).str.extract(r'(\d{4})')[0]
                    return df.sort_values("æ’å").head(limit)
        return pd.DataFrame(rows).sort_values("æ’å").head(limit)
    except Exception as e:
        st.error(f"æŠ“å–å¸‚å€¼æ’åå¤±æ•—: {e}"); return pd.DataFrame()

@st.cache_data(ttl=3600)
def fetch_msci_list():
    # (ç¶­æŒåŸæ¨£)
    url = "https://stock.capital.com.tw/z/zm/zmd/zmdc.djhtm?MSCI=0"
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20, verify=False)
        guess = chardet.detect(resp.content)
        encoding = guess['encoding'] if guess['encoding'] else 'cp950'
        html_text = resp.content.decode(encoding, errors="ignore")
        codes = set(re.findall(r"Link2Stk\('(\d{4})'\)", html_text))
        if not codes: codes = set(re.findall(r"\b(\d{4})\b", BeautifulSoup(html_text, "lxml").get_text()))
        return sorted(list(codes))
    except: return []

@st.cache_data(ttl=3600)
def fetch_0050_holdings():
    # (ç¶­æŒåŸæ¨£)
    url = "https://www.moneydj.com/ETF/X/Basic/Basic0007a.xdjhtm?etfid=0050.TW"
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20, verify=False)
        resp.encoding = resp.apparent_encoding or "utf-8"
        dfs = pd.read_html(io.StringIO(resp.text), flavor="lxml")
        names = []
        for df in dfs:
            cols = [str(c[-1] if isinstance(df.columns, pd.MultiIndex) else c).strip() for c in df.columns]
            df.columns = cols
            target = next((c for c in cols if "åç¨±" in c), None)
            if target: names.extend(df[target].astype(str).str.strip().tolist())
        return pd.DataFrame({"è‚¡ç¥¨åç¨±": list(set([n for n in names if n not in ['nan','']]))})
    except: return pd.DataFrame()

# --- ğŸ”¥ æ–°å¢ï¼šé€²éšè¡Œæƒ…æŠ“å– (é‡åƒ¹åˆ†æ) ---
@st.cache_data(ttl=300)
def get_advanced_stock_info(codes):
    if not codes: return {}
    try:
        tickers = " ".join([f"{c}.TW" for c in codes])
        data = yf.Tickers(tickers)
        res = {}
        for c in codes:
            try:
                t = data.tickers[f"{c}.TW"]
                # æŠ“ 5 å¤©è³‡æ–™ä»¥è¨ˆç®—æ¼²è·Œèˆ‡å¹³å‡é‡
                h = t.history(period="5d")
                if not h.empty:
                    curr_price = h["Close"].iloc[-1]
                    prev_price = h["Close"].iloc[-2] if len(h) > 1 else curr_price
                    vol = h["Volume"].iloc[-1]
                    avg_vol = h["Volume"].mean()
                    
                    # è¨ˆç®—æ¼²è·Œå¹…
                    change_pct = ((curr_price - prev_price) / prev_price) * 100
                    
                    # è¡æ“ŠæŒ‡æ¨™ï¼šå¦‚æœä»Šæ—¥é‡ > 5æ—¥å‡é‡ 2å€ -> çˆ†é‡
                    vol_status = "ğŸ”¥çˆ†é‡" if vol > (avg_vol * 2) else "ğŸ’§ç¸®é‡" if vol < (avg_vol * 0.5) else "â–æ­£å¸¸"
                    
                    # æ ¼å¼åŒ–æˆäº¤é‡ (å¼µæ•¸)
                    vol_str = f"{int(vol/1000)}å¼µ"
                    
                    res[c] = {
                        "ç¾åƒ¹": f"{curr_price:.2f}",
                        "æ¼²è·Œ": f"{change_pct:+.2f}%",
                        "é‡èƒ½": f"{vol_str} ({vol_status})",
                        "raw_vol": vol
                    }
                else:
                    res[c] = {"ç¾åƒ¹": "-", "æ¼²è·Œ": "-", "é‡èƒ½": "-", "raw_vol": 0}
            except:
                res[c] = {"ç¾åƒ¹": "-", "æ¼²è·Œ": "-", "é‡èƒ½": "-", "raw_vol": 0}
        return res
    except: return {}

# -------------------------------------------
# 3. æ™ºèƒ½æ—¥ç¨‹èˆ‡ç­–ç•¥
# -------------------------------------------
def get_strategy_calendar():
    today = date.today()
    m = today.month
    
    # 0056 / 00878 è¡Œäº‹æ›†
    etf_high_yield = {
        "00878": {"months": [5, 11], "name": "åœ‹æ³°æ°¸çºŒé«˜è‚¡æ¯"},
        "0056":  {"months": [6, 12], "name": "å…ƒå¤§é«˜è‚¡æ¯"},
        "00919": {"months": [5, 12], "name": "ç¾¤ç›Šå°ç£ç²¾é¸é«˜æ¯"}
    }
    
    active_etfs = []
    for code, info in etf_high_yield.items():
        if m in info["months"]:
            active_etfs.append(f"ğŸ”¸ {code} {info['name']}")
            
    return active_etfs

# -------------------------------------------
# 4. ä¸»ä»‹é¢
# -------------------------------------------
st.title("ğŸ“ˆ å°è‚¡æŒ‡æ•¸æˆ°æƒ…å®¤ Pro")
st.caption("è³‡æ–™ä¾†æºï¼šæœŸäº¤æ‰€ | MoneyDJ | Yahoo Finance (é‡åƒ¹åˆ†æ)")

# å´é‚Šæ¬„
with st.sidebar:
    st.header("ğŸ“… è±†è…è¡Œäº‹æ›†")
    
    # é¡¯ç¤ºé«˜è‚¡æ¯ ETF æ˜¯å¦æ­£åœ¨èª¿æ•´
    high_yield_now = get_strategy_calendar()
    if high_yield_now:
        st.markdown("### ğŸ”¥ æœ¬æœˆé«˜è‚¡æ¯ ETF æˆ°å ´")
        for item in high_yield_now:
            st.write(item)
        st.info("ç­–ç•¥ï¼šé«˜è‚¡æ¯ ETF èª¿æ•´é€šå¸¸æœƒå‰”é™¤æ®–åˆ©ç‡è®Šä½çš„è‚¡ç¥¨ï¼Œç´å…¥æ–°çš„é«˜é…æ¯è‚¡ã€‚è«‹ç•™æ„æŠ•ä¿¡è²·è³£è¶…ã€‚")
    else:
        st.markdown("### ğŸ’¤ æœ¬æœˆç„¡å¤§å‹é«˜è‚¡æ¯ ETF èª¿æ•´")
        st.text("ä¸‹æ³¢ç†±é»ï¼š12æœˆ (0056, 00919)")

    st.divider()
    if st.button("ğŸ”„ æ›´æ–°å³æ™‚è¡Œæƒ…"):
        st.cache_data.clear()
        st.rerun()

# æŠ“å–è³‡æ–™
with st.spinner("æ­£åœ¨é€²è¡Œå…¨å¸‚å ´é‡åƒ¹æƒæ..."):
    df_mcap = fetch_taifex_rankings()
    msci_codes = fetch_msci_list()
    df_0050 = fetch_0050_holdings()

if df_mcap.empty:
    st.error("ç„¡æ³•é€£ç·šè³‡æ–™æº"); st.stop()

# æ•¸æ“šè™•ç† helper
def enrich_data(df, target_codes):
    if df.empty: return df
    info = get_advanced_stock_info(target_codes)
    
    df["ç¾åƒ¹"] = df["è‚¡ç¥¨ä»£ç¢¼"].map(lambda x: info.get(x, {}).get("ç¾åƒ¹", "-"))
    df["æ¼²è·Œå¹…"] = df["è‚¡ç¥¨ä»£ç¢¼"].map(lambda x: info.get(x, {}).get("æ¼²è·Œ", "-"))
    df["æˆäº¤é‡/ç‹€æ…‹"] = df["è‚¡ç¥¨ä»£ç¢¼"].map(lambda x: info.get(x, {}).get("é‡èƒ½", "-"))
    
    # ç°¡å–®çš„æ¨£å¼è™•ç† (åˆ©ç”¨ Pandas Styler åœ¨ Streamlit é¡¯ç¤ºæœƒæ¯”è¼ƒè¤‡é›œï¼Œé€™è£¡ç›´æ¥ç”¨æ¬„ä½é¡¯ç¤º)
    return df

tab1, tab2, tab3 = st.tabs(["ğŸ‡¹ğŸ‡¼ 0050 é—œéµæˆ°å½¹", "ğŸŒ MSCI å­£åº¦èª¿æ•´", "ğŸ§  æ“ç›¤æ‰‹ç­†è¨˜"])

# ==========================================
# Tab 1: 0050
# ==========================================
with tab1:
    st.markdown("#### ğŸ¯ 0050 æ½›åœ¨èª¿æ•´åå–® (å«é‡åƒ¹åˆ†æ)")
    if not df_0050.empty:
        curr_0050 = set(df_0050["è‚¡ç¥¨åç¨±"].str.strip())
        df_anl = df_mcap.head(100).copy()
        df_anl["in_0050"] = df_anl["è‚¡ç¥¨åç¨±"].isin(curr_0050)
        
        # ç¯©é¸
        must_in = df_anl[(df_anl["æ’å"] <= 40) & (~df_anl["in_0050"])]
        candidates = df_anl[(df_anl["æ’å"] > 40) & (df_anl["æ’å"] <= 50) & (~df_anl["in_0050"])].head(3)
        
        target_codes = list(must_in["è‚¡ç¥¨ä»£ç¢¼"]) + list(candidates["è‚¡ç¥¨ä»£ç¢¼"])
        
        # é¡¯ç¤º å¿…ç„¶ç´å…¥
        if not must_in.empty:
            st.success("ğŸ”¥ **å¼·åŠ›è²·é€²è¨Šè™Ÿ (å¿…ç„¶ç´å…¥)**")
            st.markdown("é—œæ³¨é‡é»ï¼šè‹¥**æˆäº¤é‡åä½** (æµå‹•æ€§å·®)ï¼Œè¢«å‹•è²·ç›¤é€²å ´æ™‚æœƒæœ‰æ›´å¤§çš„æ¼²å¹…ã€‚")
            final_df = enrich_data(must_in.copy(), target_codes)
            st.dataframe(final_df[["æ’å", "è‚¡ç¥¨ä»£ç¢¼", "è‚¡ç¥¨åç¨±", "ç¾åƒ¹", "æ¼²è·Œå¹…", "æˆäº¤é‡/ç‹€æ…‹"]], hide_index=True)
        else:
            st.info("ç›®å‰å‰ 40 åçš†å·²åœ¨ 0050 å…§ã€‚")
            
        st.divider()
        
        # é¡¯ç¤º æŒ‘æˆ°è€…
        st.markdown("#### âš”ï¸ é—œéµæŒ‘æˆ°è€… (ç¬¬ 41-50 å)")
        st.markdown("ç­–ç•¥ï¼šè‹¥ç¬¬ 40 åä¹‹å¾Œçš„å¸‚å€¼å·®è·æ¥µå°ï¼Œå¯è³­**ã€Œæ’åé€†è½‰ã€**ã€‚è§€å¯Ÿä¸‹æ–¹æ¼²è·Œå¹…ï¼Œçœ‹èª°å‹•èƒ½å¼·ã€‚")
        if not candidates.empty:
            cand_df = enrich_data(candidates.copy(), target_codes)
            st.dataframe(cand_df[["æ’å", "è‚¡ç¥¨ä»£ç¢¼", "è‚¡ç¥¨åç¨±", "ç¾åƒ¹", "æ¼²è·Œå¹…", "æˆäº¤é‡/ç‹€æ…‹"]], hide_index=True)
            
    else:
        st.warning("ç„¡æ³•å–å¾— 0050 è³‡æ–™")

# ==========================================
# Tab 2: MSCI
# ==========================================
with tab2:
    st.markdown("#### ğŸŒ MSCI è§€å¯Ÿçœ‹æ¿")
    if msci_codes:
        # é‚è¼¯ï¼šæ‰¾å‡ºé«˜æ©Ÿç‡ç´å…¥ (æ’åå‰ 85 ä¸”ä¸åœ¨åå–®å…§)
        prob_in = df_mcap[(df_mcap["æ’å"] <= 85) & (~df_mcap["è‚¡ç¥¨ä»£ç¢¼"].isin(msci_codes))]
        
        if not prob_in.empty:
            st.success("ğŸ”¥ **MSCI æ½›åœ¨é»‘é¦¬ (å¸‚å€¼é«˜æ©Ÿç‡ç´å…¥)**")
            st.markdown("ç­–ç•¥ï¼šè‹¥åå–®å·²å…¬å¸ƒä¸”ç¢ºèªç´å…¥ï¼Œè«‹é—œæ³¨**ç”Ÿæ•ˆæ—¥å°¾ç›¤**ã€‚")
            
            final_df = enrich_data(prob_in.copy(), list(prob_in["è‚¡ç¥¨ä»£ç¢¼"]))
            st.dataframe(final_df[["æ’å", "è‚¡ç¥¨ä»£ç¢¼", "è‚¡ç¥¨åç¨±", "ç¾åƒ¹", "æ¼²è·Œå¹…", "æˆäº¤é‡/ç‹€æ…‹"]], hide_index=True)
        else:
            st.info("å‰ 85 åçš†å·²åœ¨ MSCI åå–®å…§ã€‚")
            
        # é¡¯ç¤ºé‚Šç·£è§€å¯Ÿ
        st.markdown("#### ğŸ§ é‚Šç·£è§€å¯Ÿå€ (86-100å)")
        watch = df_mcap[(df_mcap["æ’å"] > 85) & (df_mcap["æ’å"] <= 100) & (~df_mcap["è‚¡ç¥¨ä»£ç¢¼"].isin(msci_codes))]
        if not watch.empty:
            watch_df = enrich_data(watch.copy(), list(watch["è‚¡ç¥¨ä»£ç¢¼"]))
            st.dataframe(watch_df[["æ’å", "è‚¡ç¥¨ä»£ç¢¼", "è‚¡ç¥¨åç¨±", "ç¾åƒ¹", "æ¼²è·Œå¹…", "æˆäº¤é‡/ç‹€æ…‹"]], hide_index=True)
    else:
        st.warning("ç„¡æ³•å–å¾— MSCI åå–®")

# ==========================================
# Tab 3: æ“ç›¤æ‰‹ç­†è¨˜ (æ–°å¢)
# ==========================================
with tab3:
    st.markdown("""
    ### ğŸ§  è€å¸æ©Ÿçš„ ETF åƒè±†è…å¿ƒæ³•
    
    #### 1. ä»€éº¼æ˜¯ã€Œæµå‹•æ€§è¡æ“Šã€ï¼Ÿ (Liquidity Shock)
    ç•¶ 0050 é€™ç¨®å·¨å‹ ETF å¿…é ˆè²·å…¥ä¸€æª”è‚¡ç¥¨ï¼Œä½†é€™æª”è‚¡ç¥¨å¹³å¸¸æ²’ä»€éº¼äººåœ¨äº¤æ˜“ (æˆäº¤é‡ä½)ï¼ŒETF çš„è²·ç›¤æœƒæŠŠè‚¡åƒ¹ç¬é–“è²·ä¸Šå»ã€‚
    *   **è§€å¯Ÿé‡é»ï¼š** åœ¨ã€Œå¿…ç„¶ç´å…¥ã€åå–®ä¸­ï¼Œæ‰¾ **"æˆäº¤é‡/ç‹€æ…‹" ç‚º "ğŸ’§ç¸®é‡"** çš„è‚¡ç¥¨ã€‚é€™å°±æ˜¯æœ€è‚¥çš„è‚‰ã€‚
    
    #### 2. æ™‚é–“é»çš„è—è¡“
    *   **å…¬å¸ƒå‰ (çŒœé¡ŒæœŸ)ï¼š** è²·é€²é«˜æ©Ÿç‡åå–® (å¦‚æœ¬ç¶²é é æ¸¬çš„ Rank <= 40)ã€‚è³ºçš„æ˜¯ã€Œå¸‚å ´é æœŸã€çš„éŒ¢ã€‚
    *   **å…¬å¸ƒå¾Œ~ç”Ÿæ•ˆå‰ (æŠ¬è½æœŸ)ï¼š** æ•£æˆ¶çœ‹åˆ°æ–°èé€²å ´ï¼Œè‚¡åƒ¹æœƒæ¼²ã€‚æ­¤æ™‚æ˜¯è³£é»ï¼Œä¸æ˜¯è²·é»ã€‚
    *   **ç”Ÿæ•ˆæ—¥ (æ±ºæˆ°æ—¥)ï¼š** 
        *   **å°¾ç›¤çˆ†é‡**ï¼šETF æœƒåœ¨ 13:25~13:30 æ›å¸‚åƒ¹è²·é€²ã€‚
        *   **ç­–ç•¥**ï¼šå¦‚æœä½ æ‰‹ä¸Šæœ‰è²¨ï¼Œæ›æ¼²åœæ¿è³£çµ¦ ETFï¼›å¦‚æœä½ æƒ³ç•¶æ²–ï¼Œå°¾ç›¤å‰å…ˆæ‹‰é«˜å‡ºè²¨ã€‚
    
    #### 3. å°å¿ƒã€Œé åˆ¤ä½ çš„é åˆ¤ã€
    ç¾åœ¨ ETF èª¿æ•´å¤ªé€æ˜ï¼Œå¾ˆå¤šäººæœƒææ—©å¡ä½ã€‚å¦‚æœå…¬å¸ƒåå–®å‰è‚¡åƒ¹å·²ç¶“æ¼²äº† 30%ï¼Œå…¬å¸ƒç•¶å¤©å¯èƒ½æœƒ**åˆ©å¤šå‡ºç›¡**åè€Œä¸‹è·Œã€‚
    *   **é¿é›·é‡ï¼š** çœ‹ "æ¼²è·Œå¹…"ï¼Œå¦‚æœé€²æ¦œå‰å·²ç¶“å¤§æ¼²ä¸€æ®µï¼Œåƒè¬åˆ¥è¿½ã€‚
    
    #### 4. å…¶ä»– ETF æˆ°å ´
    åˆ¥åªç›¯è‘— 0050/MSCIã€‚
    *   **00878 (5/11æœˆ)**ã€**0056 (6/12æœˆ)**ã€**00919 (5/12æœˆ)**
    *   é€™äº›é«˜è‚¡æ¯ ETF è¦æ¨¡å·¨å¤§ï¼Œèª¿æ•´æ™‚å°ä¸­å‹è‚¡ (å¸‚å€¼ 50-150 å) çš„è¡æ“ŠåŠ›æ¯” 0050 é‚„å¼·ï¼
    """)
