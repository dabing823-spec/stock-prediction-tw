import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import io
import chardet
from datetime import date, datetime
import urllib3
# å¿½ç•¥ä¸å®‰å…¨é€£ç·šçš„è­¦å‘Šè¨Šæ¯
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
# è¨­å®šé é¢æ¨™é¡Œèˆ‡é…ç½®
st.set_page_config(page_title="å°è‚¡æŒ‡æ•¸èª¿æ•´é æ¸¬ (MSCI & 0050)", layout="wide")

# ==========================================
# æ ¸å¿ƒå·¥å…·å‡½å¼ (çˆ¬èŸ²èˆ‡è§£æ)
# ==========================================

# æ¨¡æ“¬ç€è¦½å™¨ Header
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

@st.cache_data(ttl=3600) # è¨­å®šå¿«å– 1 å°æ™‚ï¼Œé¿å…é »ç¹æŠ“å–è¢«é–
def fetch_taifex_rankings(limit=200):
    """æŠ“å–æœŸäº¤æ‰€å¸‚å€¼æ’å"""
    url = "https://www.taifex.com.tw/cht/9/futuresQADetail"
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        # è‡ªå‹•åµæ¸¬ç·¨ç¢¼
        encoding = chardet.detect(resp.content)["encoding"] or resp.apparent_encoding
        html_text = resp.content.decode(encoding, errors="ignore")
        
        soup = BeautifulSoup(html_text, "lxml")
        rows = []
        
        # è§£æé‚è¼¯æ•´åˆ (ä¾æ“šåŸè…³æœ¬é‚è¼¯)
        for tr in soup.find_all("tr"):
            tds = tr.find_all("td")
            if not tds: continue
            
            rank, code, name = None, None, None
            txts = [td.get_text(strip=True) for td in tds]
            
            # å˜—è©¦æš´åŠ›è§£æï¼šæ‰¾ç´”æ•¸å­— -> 4ç¢¼ä»£è™Ÿ -> ä¸­æ–‡åç¨±
            for s in txts:
                if rank is None and re.fullmatch(r"\d+", s):
                    rank = int(s)
                elif rank is not None and code is None and re.fullmatch(r"\d{4}", s):
                    code = s
                elif rank is not None and code is not None and name is None:
                    if not re.fullmatch(r"\d+", s): # æ’é™¤æ•¸å­—
                        name = s
                        break
            
            if rank and code and name:
                rows.append({"æ’å": rank, "è‚¡ç¥¨ä»£ç¢¼": code, "è‚¡ç¥¨åç¨±": name})
        
        if not rows:
            # å‚™ç”¨æ–¹æ¡ˆï¼špandas read_html
            dfs = pd.read_html(io.StringIO(html_text), flavor=["lxml", "html5lib"])
            for df in dfs:
                # ç°¡å–®åˆ¤æ–·æ¬„ä½
                cols = "".join([str(c) for c in df.columns])
                if "æ’å" in cols and ("åç¨±" in cols or "ä»£è™Ÿ" in cols):
                    # æ¸…æ•´æ¬„ä½åç¨±
                    df.columns = [str(c).replace(" ", "") for c in df.columns]
                    # æ˜ å°„æ¬„ä½
                    col_map = {}
                    for c in df.columns:
                        if "æ’å" in c: col_map[c] = "æ’å"
                        elif "ä»£" in c: col_map[c] = "è‚¡ç¥¨ä»£ç¢¼"
                        elif "å" in c: col_map[c] = "è‚¡ç¥¨åç¨±"
                    
                    if "æ’å" in col_map.values():
                        df = df.rename(columns=col_map)
                        # è½‰å‹è™•ç†
                        df = df[pd.to_numeric(df["æ’å"], errors='coerce').notnull()]
                        df["æ’å"] = df["æ’å"].astype(int)
                        df["è‚¡ç¥¨ä»£ç¢¼"] = df["è‚¡ç¥¨ä»£ç¢¼"].astype(str).str.extract(r'(\d{4})')[0]
                        return df.sort_values("æ’å").head(limit)

        df = pd.DataFrame(rows)
        return df.sort_values("æ’å").head(limit)
        
    except Exception as e:
        st.error(f"æŠ“å–å¸‚å€¼æ’åå¤±æ•—: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=3600)
def fetch_msci_list():
    @st.cache_data(ttl=3600)
def fetch_msci_list():
    """æŠ“å– MSCI æˆåˆ†è‚¡"""
    url = "https://stock.capital.com.tw/z/zm/zmd/zmdc.djhtm?MSCI=0"
    try:
        # æ³¨æ„é€™è£¡å¤šäº† verify=False
        resp = requests.get(url, headers=HEADERS, timeout=20, verify=False)
        
        # (ä»¥ä¸‹ç¨‹å¼ç¢¼ä¿æŒä¸è®Š...)
        guess = chardet.detect(resp.content)
        encoding = guess['encoding'] if guess['encoding'] else 'cp950'
        html_text = resp.content.decode(encoding, errors="ignore")
        
        codes = set()
        for m in re.finditer(r"Link2Stk\('(\d{4})'\)", html_text):
            codes.add(m.group(1))
        
        if not codes:
            soup = BeautifulSoup(html_text, "lxml")
            txt = soup.get_text()
            for m in re.finditer(r"\b(\d{4})\b", txt):
                codes.add(m.group(1))
                
        return sorted(list(codes))
    except Exception as e:
        st.error(f"æŠ“å– MSCI åå–®å¤±æ•—: {e}")
        return []

@st.cache_data(ttl=3600)
def fetch_0050_holdings():
    """æŠ“å– MoneyDJ 0050 æŒè‚¡"""
    url = "https://www.moneydj.com/ETF/X/Basic/Basic0007a.xdjhtm?etfid=0050.TW"
    try:
        # æ³¨æ„é€™è£¡å¤šäº† verify=False
        resp = requests.get(url, headers=HEADERS, timeout=20, verify=False)
        resp.encoding = resp.apparent_encoding or "utf-8"
        
        # (ä»¥ä¸‹ç¨‹å¼ç¢¼ä¿æŒä¸è®Š...)
        dfs = pd.read_html(io.StringIO(resp.text), flavor="lxml")
        all_names = []
        
        for df in dfs:
            if isinstance(df.columns, pd.MultiIndex):
                cols = [str(c[-1]).strip() for c in df.columns]
            else:
                cols = [str(c).strip() for c in df.columns]
            
            df.columns = cols
            
            target_col = None
            for c in cols:
                if "åç¨±" in c:
                    target_col = c
                    break
            
            if target_col:
                names = df[target_col].astype(str).str.strip().tolist()
                all_names.extend([n for n in names if n != 'nan' and n != ''])
        
        unique_names = list(set(all_names))
        return pd.DataFrame({"è‚¡ç¥¨åç¨±": unique_names})
        
    except Exception as e:
        st.error(f"æŠ“å– 0050 åå–®å¤±æ•—: {e}")
        return pd.DataFrame()

# ==========================================
# ä»‹é¢é‚è¼¯
# ==========================================

st.title("ğŸ“Š å°ç£è‚¡å¸‚æŒ‡æ•¸èª¿æ•´é æ¸¬å·¥å…·")
st.markdown("è³‡æ–™ä¾†æºï¼šæœŸäº¤æ‰€ (å¸‚å€¼æ’å)ã€ç¾¤ç›Šè­‰åˆ¸ (MSCI)ã€MoneyDJ (0050)")

# å´é‚Šæ¬„ï¼šé‡æ–°æ•´ç†æŒ‰éˆ•
if st.sidebar.button("ğŸ”„ å¼·åˆ¶æ›´æ–°è³‡æ–™"):
    st.cache_data.clear()
    st.rerun()

st.sidebar.info(f"è³‡æ–™å¿«å–æ™‚é–“ï¼š{datetime.now().strftime('%H:%M:%S')}")

# --- æº–å‚™è³‡æ–™ ---
with st.spinner("æ­£åœ¨å¾äº¤æ˜“æ‰€èˆ‡è²¡ç¶“ç¶²ç«™æŠ“å–æœ€æ–°è³‡æ–™..."):
    df_mcap = fetch_taifex_rankings(limit=200)
    
    if df_mcap.empty:
        st.error("ç„¡æ³•å–å¾—å¸‚å€¼æ’åè³‡æ–™ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")
        st.stop()

    msci_codes = fetch_msci_list()
    df_0050 = fetch_0050_holdings()

# --- åˆ†é é¡¯ç¤º ---
tab1, tab2 = st.tabs(["ğŸŒ MSCI ç´å…¥/å‰”é™¤è§€å¯Ÿ", "ğŸ‡¹ğŸ‡¼ 0050 æˆåˆ†è‚¡å¯©æ ¸"])

# =======================
# Tab 1: MSCI åˆ†æ
# =======================
with tab1:
    st.subheader("MSCI å­£åº¦èª¿æ•´è§€å¯Ÿ (ä»¥å¸‚å€¼æ’åäº¤å‰æ¯”å°)")
    
    # æç¤ºè¨Šæ¯
    today = date.today()
    if today.month in [2, 5, 8, 11]:
        st.warning(f"ğŸ—“ï¸ æœ¬æœˆ ({today.month}æœˆ) ç‚º MSCI å¯©æ ¸æœˆä»½ï¼è«‹å¯†åˆ‡æ³¨æ„ã€‚")
    
    if msci_codes:
        # é‚è¼¯è™•ç†
        # 1. å‰ 100 å ä½†ä¸åœ¨ MSCI (æ½›åœ¨ç´å…¥)
        cond1 = df_mcap[df_mcap["æ’å"] <= 100].copy()
        cond1 = cond1[~cond1["è‚¡ç¥¨ä»£ç¢¼"].isin(msci_codes)].reset_index(drop=True)
        
        # 2. 100 åå¾Œ ä½†åœ¨ MSCI (æ½›åœ¨å‰”é™¤é¢¨éšª)
        cond2 = df_mcap[df_mcap["æ’å"] > 100].copy()
        cond2 = cond2[cond2["è‚¡ç¥¨ä»£ç¢¼"].isin(msci_codes)].reset_index(drop=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ğŸš€ æ½›åœ¨ç´å…¥è§€å¯Ÿ (å‰100æœªå…¥åˆ—)")
            st.dataframe(cond1, use_container_width=True)
            
        with col2:
            st.markdown("#### âš ï¸ æ½›åœ¨å‰”é™¤è§€å¯Ÿ (æ’å>100ä»åœ¨åˆ—)")
            st.dataframe(cond2, use_container_width=True)
            
        # Excel ä¸‹è¼‰
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            cond1.to_excel(writer, sheet_name='æ½›åœ¨ç´å…¥', index=False)
            cond2.to_excel(writer, sheet_name='æ½›åœ¨å‰”é™¤', index=False)
            df_mcap.to_excel(writer, sheet_name='å¸‚å€¼å‰200', index=False)
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ MSCI åˆ†æå ±å‘Š (Excel)",
            data=output.getvalue(),
            file_name=f"MSCI_Report_{today.strftime('%Y%m%d')}.xlsx",
            mime="application/vnd.ms-excel"
        )
    else:
        st.warning("ç„¡æ³•å–å¾—ç›®å‰çš„ MSCI åå–®ï¼Œåƒ…é¡¯ç¤ºå¸‚å€¼æ’åã€‚")
        st.dataframe(df_mcap)

# =======================
# Tab 2: 0050 åˆ†æ
# =======================
with tab2:
    st.subheader("0050 æˆåˆ†è‚¡èª¿æ•´é æ¸¬")
    st.markdown("""
    **è¦å‰‡åƒè€ƒï¼š**
    - **å¿…å®šç´å…¥**ï¼šå¸‚å€¼æ’å <= 40 ä¸”ç›®å‰ä¸åœ¨ 0050
    - **å¿…å®šå‰”é™¤**ï¼šå¸‚å€¼æ’å > 60 ä¸”ç›®å‰åœ¨ 0050
    - **è§€å¯Ÿå€**ï¼šæ’å 41~60 ä¹‹é–“ (è¦–æŠ€è¡“è¦å‰‡æ±ºå®š)
    """)
    
    if not df_0050.empty:
        # è³‡æ–™å‰è™•ç†
        current_0050_names = set(df_0050["è‚¡ç¥¨åç¨±"].astype(str).str.strip())
        
        df_0050_analysis = df_mcap.head(100).copy() # å–å‰100ä¾†åˆ†æå³å¯
        df_0050_analysis["æ˜¯å¦åœ¨0050ä¸­"] = df_0050_analysis["è‚¡ç¥¨åç¨±"].apply(
            lambda x: "âœ… æ˜¯" if x in current_0050_names else "âŒ å¦"
        )
        
        # é‚è¼¯ç¯©é¸
        # A. å¿…å®šé€²å…¥ (Rank <= 40 & Not in 0050)
        must_in = df_0050_analysis[
            (df_0050_analysis["æ’å"] <= 40) & 
            (df_0050_analysis["æ˜¯å¦åœ¨0050ä¸­"] == "âŒ å¦")
        ]
        
        # B. å¿…å®šå‰”é™¤ (Rank > 60 & In 0050)
        # æ³¨æ„ï¼šé€™è£¡éœ€è¦ç¢ºä¿æˆ‘å€‘æœ‰æŠ“åˆ°å¤ å¾Œé¢çš„æ’åï¼Œdf_mcap æˆ‘å€‘æŠ“äº† 200 ç­†ï¼Œå¤ ç”¨äº†
        # ä½†è¦å…ˆæ‰¾å‡ºç›®å‰åœ¨ 0050 ä½†æ’åæ‰åˆ° 60 ä»¥å¤–çš„
        
        # å…ˆæ‰¾å‡ºæ‰€æœ‰åœ¨ 0050 çš„è‚¡ç¥¨ç›®å‰çš„æ’å
        in_0050_df = df_mcap[df_mcap["è‚¡ç¥¨åç¨±"].isin(current_0050_names)].copy()
        must_out = in_0050_df[in_0050_df["æ’å"] > 60]
        
        # C. é‚Šç·£è§€å¯Ÿ (æ’å 41-60)
        edge_watch = df_0050_analysis[
            (df_0050_analysis["æ’å"] > 40) & 
            (df_0050_analysis["æ’å"] <= 60)
        ]
        
        # é¡¯ç¤ºçµæœ
        c1, c2 = st.columns(2)
        with c1:
            st.error("#### ğŸ‘‹ é æ¸¬å‰”é™¤åå–® (æ’å > 60)")
            if must_out.empty:
                st.info("ç›®å‰æ²’æœ‰ç¬¦åˆå¿…å®šå‰”é™¤è¦å‰‡çš„å€‹è‚¡")
            else:
                st.dataframe(must_out[["æ’å", "è‚¡ç¥¨ä»£ç¢¼", "è‚¡ç¥¨åç¨±"]], use_container_width=True)

        with c2:
            st.success("#### ğŸ‰ é æ¸¬ç´å…¥åå–® (æ’å <= 40)")
            if must_in.empty:
                st.info("ç›®å‰æ²’æœ‰ç¬¦åˆå¿…å®šç´å…¥è¦å‰‡çš„å€‹è‚¡")
            else:
                st.dataframe(must_in[["æ’å", "è‚¡ç¥¨ä»£ç¢¼", "è‚¡ç¥¨åç¨±"]], use_container_width=True)
        
        st.markdown("#### ğŸ§ é‚Šç·£è§€å¯Ÿå€ (æ’å 41 ~ 60)")
        st.dataframe(edge_watch, use_container_width=True)

        # Excel ä¸‹è¼‰
        output_0050 = io.BytesIO()
        with pd.ExcelWriter(output_0050, engine='xlsxwriter') as writer:
            must_in.to_excel(writer, sheet_name='é æ¸¬ç´å…¥', index=False)
            must_out.to_excel(writer, sheet_name='é æ¸¬å‰”é™¤', index=False)
            edge_watch.to_excel(writer, sheet_name='é‚Šç·£è§€å¯Ÿ', index=False)
            df_0050.to_excel(writer, sheet_name='ç›®å‰0050æˆåˆ†', index=False)
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ 0050 åˆ†æå ±å‘Š (Excel)",
            data=output_0050.getvalue(),
            file_name=f"0050_Predict_{today.strftime('%Y%m%d')}.xlsx",
            mime="application/vnd.ms-excel"
        )
        
    else:
        st.warning("ç„¡æ³•å–å¾— 0050 æˆåˆ†è‚¡è³‡æ–™ï¼Œç„¡æ³•é€²è¡Œäº¤å‰åˆ†æã€‚")
